{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9eeb87-10a0-4936-a86b-e65b49dc8e23",
   "metadata": {},
   "source": [
    "# ReRank with TinyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4623992-28a7-4551-8053-bf2c673a8215",
   "metadata": {},
   "source": [
    "### Step 0: Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c5cdf7-a280-48ba-9bb7-07c16d5c8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import persist_and_normalize_run, load_rerank_data\n",
    "from sentence_transformers import CrossEncoder\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9193c43-ede4-4130-b766-7d9047f52faf",
   "metadata": {},
   "source": [
    "### Step 1: Load the re-rank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea00c096-f1ae-408b-b24d-9ad7ff46fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will use a small hardcoded example located in reneuir-2024/re-rank-spot-check-20240624-training.\n"
     ]
    }
   ],
   "source": [
    "# In the TIRA sandbox, this is the injected re-ranking dataset, injected via the environment variable TIRA_INPUT_DIRECTORY\n",
    "re_rank_dataset = load_rerank_data(default='reneuir-2024/re-rank-spot-check-20240624-training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f68b48-28f1-44f3-9ede-532b15d9b15a",
   "metadata": {},
   "source": [
    "### Step 3: Load the model\n",
    "\n",
    "We pass the model via an environment variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b0800a-54f9-4458-90d7-f68fdefe2304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will use the model webis/tiny-bert-ranker-l2-bm25.\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_MODEL = 'webis/tiny-bert-ranker-l2-bm25'\n",
    "model_name = os.environ.get('MODEL', DEFAULT_MODEL)\n",
    "print(f'I will use the model {model_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "081f07c8-9eb3-44fb-97bb-08f9f11e1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder(model_name, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e91df45-faf8-40f4-886b-45f4bbb84785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is: <sentence_transformers.cross_encoder.CrossEncoder.CrossEncoder object at 0x7f9b8466eb00>\n"
     ]
    }
   ],
   "source": [
    "print('Model is:', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3eabb4-d2a1-4d98-a665-0e3d5b6e8651",
   "metadata": {},
   "source": [
    "### Step 4: Make all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3cf665c-7aaa-4e7c-a82a-a0790aabe61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transform dataset for predictions: 500it [00:00, 5254.20it/s]\n"
     ]
    }
   ],
   "source": [
    "global_min_bm25 = 0\n",
    "global_max_bm25 = 0\n",
    "\n",
    "for _, i in re_rank_dataset.iterrows():\n",
    "  if i['score'] > global_max_bm25:\n",
    "    global_max_bm25 = i['score']\n",
    "\n",
    "input_data = []\n",
    "\n",
    "for _, i in tqdm(re_rank_dataset.iterrows(), 'Transform dataset for predictions'):\n",
    "    normalized_score = (i['score'] - global_min_bm25) / (global_max_bm25 - global_min_bm25)\n",
    "    normalized_score = int(normalized_score * 100)\n",
    "\n",
    "    input_data += [[\"{} [SEP] {}\".format(normalized_score, i['query']), i['text']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6c6510f-d284-4a9b-ba5c-9d1610248e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make predictions\n"
     ]
    }
   ],
   "source": [
    "print('Make predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be446d28-d2b8-4606-933a-c641be261988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "re_rank_dataset['score'] = model.predict(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776c0b7-45d5-414d-98a0-142fff6c50f1",
   "metadata": {},
   "source": [
    "### Step 5: Write run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "782f366c-6452-437e-9d3d-ad95ca1ca1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \"./run.txt\".\n",
      "Done. run file is stored under \"./run.txt\".\n"
     ]
    }
   ],
   "source": [
    "persist_and_normalize_run(re_rank_dataset, model_name, default_output='./run.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
