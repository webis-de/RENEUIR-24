@article{cui21,
  author       = {Baiyun Cui and Yingming Li and Zhongfei Zhang},
  title        = {Joint structured pruning and dense knowledge distillation for efficient transformer model compression},
  journal      = {Neurocomputing},
  volume       = {458},
  pages        = {56--69},
  year         = {2021},
  url          = {https://doi.org/10.1016/j.neucom.2021.05.084},
  doi          = {10.1016/J.NEUCOM.2021.05.084}
}

@inproceedings{jiao20,
  author       = {Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
  editor       = {Trevor Cohn and Yulan He and Yang Liu},
  title        = {TinyBERT: Distilling {BERT} for Natural Language Understanding},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2020, Online Event, 16-20 November 2020},
  series       = {Findings of {ACL}},
  volume       = {{EMNLP} 2020},
  pages        = {4163--4174},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.findings-emnlp.372},
  doi          = {10.18653/V1/2020.FINDINGS-EMNLP.372}
}

@inproceedings{wang21,
  author       = {Wenhui Wang and Hangbo Bao and Shaohan Huang and Li Dong and Furu Wei},
  editor       = {Chengqing Zong and Fei Xia and Wenjie Li and Roberto Navigli},
  title        = {MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers},
  booktitle    = {Findings of the Association for Computational Linguistics: {ACL/IJCNLP} 2021, Online Event, August 1-6, 2021},
  series       = {Findings of {ACL}},
  volume       = {{ACL/IJCNLP} 2021},
  pages        = {2140--2151},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.findings-acl.188},
  doi          = {10.18653/V1/2021.FINDINGS-ACL.188},
}

