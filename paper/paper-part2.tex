\section{Related Work}

{\color{red} Version 0 von Max: Auch subsections zu Score Injection ...}

Why have we choosen model X and model Y for our work?

From~\cite{jiao20}: There are many advantages to TinyBERT compared to others, especially in efficiency and performance. Among them, one significant benefit is the reduced size with faster inference time: TinyBERT4, which contains only four layers, is 7.5 times smaller with 9.4 times faster than its teacher model, BERTBASE. Even with this massive drop in model size, TinyBERT4 can achieve more than 96.8\% of the performance compared to BERTBASE on all GLUE benchmark tasks, which proves that it can hold a high degree of accuracy while being substantially more efficient. In that case, TinyBERT will be particularly fitting for applications executed on devices under highly resource-restrictive conditions with limited computational power and memory.

Another major innovation of TinyBERT, however, is a new two-stage learning framework that includes general and task-specific distillation. The first is the general distillation stage, in which TinyBERT learns from the original BERT for general domain knowledge mastery.  After that, it will conduct task-specific distillation with the fine-tuned BERT for some downstream tasks so that task-specific knowledge can be absorbed into TinyBERT. In this two-phase process, the TinyBERT inherits all the rich, general knowledge from BERT and can further enhance its performance on specific tasks, making it more adaptable and effective in various natural language processing tasks.

More than that, TinyBERT's advanced distillation techniques include attention-based and hidden states-based ones. Specifically, these methods have been designed to distill the linguistic knowledge the teacher model has encoded in its attention weights and hidden states into the student model. When distilling, this technique enables TinyBERT to retain all essential features concerning syntax and coreference resolution tasks. The complementarity of these distillation techniques makes it possible for TinyBERT, with fewer parameters and reduced computational cost, to realize high performance. Finally, TinyBERT shows better performance compared with the other state-of-the-art models mainly designed for model compression. As shown, TinyBERT4 significantly outperforms other 4-layer models like BERT4-PKD and DistilBERT4 by at least 4.4\% while having only about 28\% of parameters and 31\% of inference time compared to these models. When scaled up to six layers, TinyBERT6 is on par with BERTBASE in performance; therefore, this makes it a relatively robust and scalable solution for a wide array of NLP tasks. Of course, all these advantages make TinyBERT one of the best choices for an application requiring efficient, high-performance language models.\\


Examples further include ``production ready systems'' like NBoost for ElasticSearch.\footnote{\url{https://github.com/koursaros-ai/nboost}}


- NBoost is a scalable, search-api-boosting platform for deploying
transformer models to improve the relevance of search results or
diffe rent platforms (i.e. Elasticsearch) Topics
- maybe more useful for the user pipeline
- is between user and the actual pipeline where it picks the best
based on relevance and training

JMC and DISP~\cite{cui21}:
- Joint Model Compression (JMC)
- Direct Importance-aware Structured Pruning
- Dense Knowledge Distillation (DKD)
- Paper at thulb

MiniLMv2~\cite{wang21} (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
- found in https://arxiv.org/abs/2012.15828 (MiniLMv2)
- they state that their model outperform state of the art models
- uses deep distillated self-attention which are distributed and also
give values to relations to copy teacher self-attention
- transfers knowledge of upper-middle- to last-layer(MiniLM only
mimics the last layer)

Transit(https://arxiv.org/abs/2101.03289)
