\section{Related Work}

Why have we choosen model X and model Y for our work?

From~\cite{jiao20}: TinyBERT offers several advantages over other models, especially when it comes to efficiency and performance. One of the primary benefits is its reduced size and faster inference time. TinyBERT4, which comprises only four layers, is 7.5 times smaller and 9.4 times faster than its teacher model, BERTBASE. Despite this significant reduction in size, TinyBERT4 achieves more than 96.8\% of the performance of BERTBASE on the GLUE benchmark tasks, demonstrating that it can maintain high accuracy while being substantially more efficient. This makes TinyBERT particularly suitable for applications on resource-restricted devices where computational power and memory are limited.

Furthermore, TinyBERT employs a novel two-stage learning framework that involves both general distillation and task-specific distillation. In the general distillation stage, TinyBERT learns from the original BERT model without fine-tuning, capturing general domain knowledge. Subsequently, task-specific distillation is performed using fine-tuned BERT, allowing TinyBERT to absorb knowledge tailored to specific tasks. This two-stage process ensures that TinyBERT not only inherits the rich, general knowledge from BERT but also hones its performance on specific tasks, thereby improving its adaptability and effectiveness in various natural language processing tasks.

Moreover, TinyBERT's distillation process includes advanced techniques such as attention-based and hidden states-based distillation. These techniques are designed to transfer the linguistic knowledge encoded in the teacher model's attention weights and hidden states to the student model. This method helps TinyBERT retain essential linguistic features, which are crucial for tasks involving syntax and coreference resolution. The complementary nature of these distillation methods ensures that TinyBERT can achieve high performance even with fewer parameters and reduced computational requirements.

Finally, TinyBERT demonstrates superior performance compared to other state-of-the-art models designed for model compression. For instance, TinyBERT4 significantly outperforms other 4-layer models like BERT4-PKD and DistilBERT4 by at least 4.4\%, while using only about 28\% of the parameters and 31\% of the inference time of these models . When scaled up to six layers, TinyBERT6 achieves performance on par with BERTBASE, making it a robust and scalable solution for diverse NLP tasks. These benefits collectively highlight why TinyBERT is an excellent choice for applications requiring efficient and high-performing language models.\\


Examples further include ``production ready systems'' like NBoost for ElasticSearch.\footnote{\url{https://github.com/koursaros-ai/nboost}}


- NBoost is a scalable, search-api-boosting platform for deploying
transformer models to improve the relevance of search results or
diffe rent platforms (i.e. Elasticsearch) Topics
- maybe more useful for the user pipeline
- is between user and the actual pipeline where it picks the best
based on relevance and training

JMC and DISP~\cite{cui21}:
- Joint Model Compression (JMC)
- Direct Importance-aware Structured Pruning
- Dense Knowledge Distillation (DKD)
- Paper at thulb

MiniLMv2~\cite{wang21} (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
- found in https://arxiv.org/abs/2012.15828 (MiniLMv2)
- they state that their model outperform state of the art models
- uses deep distillated self-attention which are distributed and also
give values to relations to copy teacher self-attention
- transfers knowledge of upper-middle- to last-layer(MiniLM only
mimics the last layer)

Transit(https://arxiv.org/abs/2101.03289)
